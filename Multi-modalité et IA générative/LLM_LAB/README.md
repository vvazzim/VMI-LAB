# Lab â€” Large Language Models (LLM)

**Master 2 VMI â€” Multi-modalitÃ© et IA gÃ©nÃ©rative (IFLCE055)**

Ce dossier contient le TP sur l'adaptation de modÃ¨les de langage prÃ©-entraÃ®nÃ©s (LLMs) pour une tÃ¢che de **classification de sentiments**.

---

## ğŸš€ Lancer sur Google Colab

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1ZTCXM_edq0ysQlmLN5ZBcO8p9mmLQL5w?usp=sharing)

---

## ğŸ¯ Objectifs du TP

Explorer et comparer **trois stratÃ©gies** d'adaptation d'un LLM Ã  une tÃ¢che aval :

| StratÃ©gie | Description |
|-----------|-------------|
| **Inference without training** | Utilisation du modÃ¨le prÃ©-entraÃ®nÃ© tel quel (zÃ©ro entraÃ®nement) |
| **Linear Probing** | Gel du backbone, entraÃ®nement uniquement de la tÃªte de classification |
| **Fine-tuning** | Complet (tous les paramÃ¨tres) ou partiel (certaines couches gelÃ©es) |

Une section dÃ©diÃ©e Ã  l'**analyse des tokens, IDs et embeddings** est Ã©galement incluse.

---

## ğŸ§  ModÃ¨le et donnÃ©es

- **Dataset :** `cornell-movie-review-data/rotten_tomatoes` (Hugging Face)
  - Train : 8 530 | Validation : 1 066 | Test : 1 066
- **ModÃ¨le :** `cardiffnlp/twitter-roberta-base-sentiment-latest` (RoBERTa)
- **TÃ¢che :** Classification binaire (NEGATIVE / POSITIVE)

---

## ğŸ“ Structure du projet

```
LLM_LAB/
â”œâ”€â”€ notebook/
â”‚   â””â”€â”€ LLM_Lab.ipynb           # Notebook principal
â”œâ”€â”€ report/
â”‚   â””â”€â”€ TP_LLM.pdf              # Rapport final
â””â”€â”€ README.md
```

---

## ğŸ“Š RÃ©sultats (jeu de test)

| Approche | Accuracy | F1-score |
|----------|----------|----------|
| Inference sans entraÃ®nement | 0.751 | 0.742 |
| Linear probing | 0.836 | 0.835 |
| Fine-tuning complet | **0.877** | **0.874** |
| Fine-tuning partiel | 0.875 | 0.872 |

Le fine-tuning partiel offre un excellent compromis entre performance et coÃ»t computationnel.

---

## âš™ï¸ DÃ©pendances

- `transformers`, `datasets`
- `torch`, `scikit-learn`, `matplotlib`

Datasets et modÃ¨les publics â€” aucun token Hugging Face requis.

---

## ğŸ‘¤ Auteur

**Wassim Chikhi** â€” Master 2 VMI â€” UniversitÃ© Paris CitÃ© â€” 2025/2026
