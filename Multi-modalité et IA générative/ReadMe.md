# Multi-modalitÃ© et IA gÃ©nÃ©rative  
## Master 2 Vision et Machines Intelligentes (VMI) â€” UniversitÃ© Paris CitÃ©

Ce dossier regroupe les travaux du module **Multi-modalitÃ© et IA gÃ©nÃ©rative** (IFLCE055), couvrant l'adaptation de LLMs, l'apprentissage multimodal texte-image (CLIP) et la gÃ©nÃ©ration d'images (VAE, GAN).

---

## ğŸ¯ Objectif du module

Explorer les architectures et mÃ©thodes Ã  la frontiÃ¨re de la **vision par ordinateur**, du **traitement du langage** et de la **gÃ©nÃ©ration d'images** :

- Adaptation de modÃ¨les de langage prÃ©-entraÃ®nÃ©s (LLM) Ã  des tÃ¢ches aval
- Apprentissage multimodal pour l'alignement texte-image (CLIP)
- ModÃ¨les gÃ©nÃ©ratifs : VAE et GAN (DCGAN)

---

## ğŸ—‚ Structure du module

```
Multi-modalitÃ© et IA gÃ©nÃ©rative/
â”‚
â”œâ”€â”€ ReadMe.md                   # Ce fichier
â”‚
â”œâ”€â”€ LLM_LAB/                    # TP â€” Large Language Models
â”‚   â”œâ”€â”€ notebook/
â”‚   â”‚   â””â”€â”€ LLM_Lab.ipynb       # Classification de sentiments (RoBERTa)
â”‚   â”œâ”€â”€ report/
â”‚   â”‚   â””â”€â”€ TP_LLM.pdf
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ TP2 CLIP/                   # TP â€” CLIP : multimodal texte-image
â”‚   â”œâ”€â”€ notebook/
â”‚   â”‚   â””â”€â”€ TP_CLIP_Wassim.ipynb
â”‚   â”œâ”€â”€ figures/                # SimilaritÃ©s, comparaisons modÃ¨les
â”‚   â”œâ”€â”€ repport/
â”‚   â”‚   â””â”€â”€ TP2_CLIP_r.pdf
â”‚   â””â”€â”€ README.md
â”‚
â””â”€â”€ TP 3 GaN-VAE/               # TP â€” GÃ©nÃ©ration (VAE + GAN)
    â”œâ”€â”€ ganlab-completed-by-wassim.ipynb
    â”œâ”€â”€ TP_GaN_VAE_WASSIM (1).pdf
    â””â”€â”€ README.md
```

---

## ğŸ“š TPs rÃ©alisÃ©s

### ğŸ¤– LLM Lab â€” Classification de sentiments

**Objectif :** Adapter un LLM (RoBERTa) Ã  une tÃ¢che de classification binaire (NEGATIVE/POSITIVE) sur des critiques de films.

**StratÃ©gies comparÃ©es :** Inference sans entraÃ®nement, Linear Probing, Fine-tuning (complet et partiel).

| Approche | Accuracy | F1-score |
|----------|----------|----------|
| Inference sans entraÃ®nement | 0.751 | 0.742 |
| Linear probing | 0.836 | 0.835 |
| Fine-tuning complet | **0.877** | **0.874** |
| Fine-tuning partiel | 0.875 | 0.872 |

ğŸ“ Dossier : [`LLM_LAB/`](LLM_LAB/) | Rapport : `LLM_LAB/report/TP_LLM.pdf`

---

### ğŸ–¼ TP2 â€” CLIP : multimodal texte-image

**Objectif :** Explorer CLIP pour l'alignement images-textes, l'Ã©valuation sur donnÃ©es mÃ©dicales (ROCO) et la classification zero-shot.

**Contenu :** Images naturelles, fine-tuning ViT-B/16 sur ROCO, comparaison de 5 architectures, zero-shot CIFAR10.

| ExpÃ©rience | RÃ©sultat |
|------------|----------|
| Natural Images | Acc@1 = 100%, Acc@5 = 100% |
| CIFAR10 zero-shot | Top-1 = 89.16 %, Top-5 = 99.08 % |

ğŸ“ Dossier : [`TP2 CLIP/`](TP2%20CLIP/) | Rapport : `TP2 CLIP/repport/TP2_CLIP_r.pdf`

---

### ğŸ¨ TP3 â€” GAN & VAE : gÃ©nÃ©ration de sprites PokÃ©mon

**Objectif :** ImplÃ©menter un VAE convolutionnel et un GAN (DCGAN) pour gÃ©nÃ©rer des sprites PokÃ©mon.

**Contenu :** VAE (reconstruction + Ã©chantillonnage), DCGAN, Ã©tude des hyperparamÃ¨tres, comparaison qualitative.

ğŸ“ Dossier : [`TP 3 GaN-VAE/`](TP%203%20GaN-VAE/) | Rapport : `TP 3 GaN-VAE/TP_GaN_VAE_WASSIM (1).pdf`

---

## âš™ï¸ Environnement

**Librairies principales :**

- `torch`, `torchvision`
- `transformers`, `datasets` (LLM, CLIP)
- `open_clip_torch` ou `clip` (TP2)
- `matplotlib`, `scikit-learn`

Les notebooks sont exÃ©cutables sur **Google Colab** (LLM, CLIP) ou en local avec GPU (GAN-VAE recommandÃ©).

---

## ğŸ‘¤ Auteur

**Wassim Chikhi**  
Master 2 VMI â€” UniversitÃ© Paris CitÃ©  
AnnÃ©e universitaire 2025â€“2026
